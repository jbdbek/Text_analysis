{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3208ed06-aeb8-449c-bbf0-8cbe39e4227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zhconv                          #导入zhconv模块\n",
    "import jieba                           #导入jieba分词模块\n",
    "import re                              #导入正则表达式模块\n",
    "import multiprocessing                 #导入多进程模块\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from enchant.checker import SpellChecker\n",
    "from gensim.corpora import WikiCorpus    #导入Wiki语料库\n",
    "from gensim.models import word2vec      #导入word2vec模型\n",
    "from gensim.utils import simple_preprocess\n",
    "from utils.configs import csv_to_txt_path\n",
    "from utils.configs import process_csv_out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298620a-7d61-472c-99c4-7092390d6c8a",
   "metadata": {},
   "source": [
    "加载停用词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2422fd47-6b2f-4674-b82f-71bd1a1fcd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(word.strip() for word in file.readlines())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef3b3ab-29f0-4bd5-8edd-26a995b594c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, stop_words):\n",
    "    text = text.strip().replace(\"\\n\", \"\").replace(\"\\r\", \"\")                                      #去除换行符和多余空格\n",
    "    words = jieba.cut(text)                                                                      #使用jieba分词\n",
    "    processed_words = [word for word in words if word not in stop_words and word.strip() != \"\"]  #去除停用词\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634c2866-5bef-4b0f-8736-9da3933c7a06",
   "metadata": {},
   "source": [
    "中文数据处理（有标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b9a6a49-159c-4177-aa87-4017e5ce6889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vector_label(input_file_path, output_file_path, stop_words_file_path):\n",
    "    cn_reg = '[\\u4e00-\\u9fa5]'                                                                   #正则表达式，匹配所有中文字符\n",
    "    stop_words = load_stop_words(stop_words_file_path)                                           #加载停用词    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        original_header = next(reader)                                                           #读取输入文件表头\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerow(['ID', 'Content', 'Label'])                                          #写入表头\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                id_column = row[0]\n",
    "                content_column = row[1]\n",
    "                label_column = row[2]\n",
    "                \n",
    "                simplified_content = zhconv.convert(content_column, 'zh-hans')                   #将内容转换为简体字\n",
    "                segmented_content = jieba.cut(simplified_content)                                #分词处理\n",
    "                filtered_words = [word for word in segmented_content if re.match(cn_reg, word)]  #去除非中文字符，仅保留中文字符\n",
    "                processed_content = [word for word in filtered_words if word not in stop_words]  #去停用词\n",
    "                cleaned_content = ' '.join(processed_content)                                       #合并为字符串\n",
    "                \n",
    "                writer.writerow([id_column, cleaned_content, label_column])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已处理 {count} 条数据')\n",
    "            print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498af31-fcee-4df0-866a-f969b69faa29",
   "metadata": {},
   "source": [
    "中文数据处理（无标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01b924b6-a355-407e-bf52-e62473fcd823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vector_nolabel(input_file_path, output_file_path, stop_words_file_path):\n",
    "    cn_reg = '[\\u4e00-\\u9fa5]'                                                                   #正则表达式，匹配所有中文字符\n",
    "    stop_words = load_stop_words(stop_words_file_path)                                           #加载停用词    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        original_header = next(reader)                                                           #读取输入文件表头\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerow(['ID', 'Content'])                                          #写入表头\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                id_column = row[0]\n",
    "                content_column = row[1]\n",
    "                \n",
    "                simplified_content = zhconv.convert(content_column, 'zh-hans')                   #将内容转换为简体字\n",
    "                segmented_content = jieba.cut(simplified_content)                                #分词处理\n",
    "                filtered_words = [word for word in segmented_content if re.match(cn_reg, word)]  #去除非中文字符，仅保留中文字符\n",
    "                processed_content = [word for word in filtered_words if word not in stop_words]  #去停用词\n",
    "                cleaned_content = ' '.join(processed_content)                                       #合并为字符串\n",
    "                \n",
    "                writer.writerow([id_column, cleaned_content])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已处理 {count} 条数据')\n",
    "            print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ebe39-4dbd-4195-ac6a-67e5f2b7f72b",
   "metadata": {},
   "source": [
    "英文数据处理（有标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9405613-832d-449e-9947-25139cbf861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_label(input_file_path, output_file_path, stop_words_file_path):\n",
    "    stop_words = load_stop_words(stop_words_file_path)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        original_header = next(reader)                                                           #读取输入文件表头\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerow(['ID', 'Content', 'Label'])                                          #写入表头\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                id_column = row[0]\n",
    "                content_column = row[1]\n",
    "                label_column = row[2]\n",
    "\n",
    "                text = re.sub(r'<.*?>', '', content_column)\n",
    "                load_text = re.sub(r'[^a-zA-Z\\s]', '', text) #去除非字母和空格的字符\n",
    "                chkr = SpellChecker(\"en_US\")\n",
    "                chkr.set_text(load_text)                                   #拼写检查纠正\n",
    "                for err in chkr:\n",
    "                    load_text = load_text.replace(err.word, err.suggest()[0] if err.suggest() else err.word)\n",
    "                #stemmer = SnowballStemmer(\"english\")\n",
    "                #stemmer_content = stemmer.stem(content_column)                                #词干提取\n",
    "                tokenized_words = load_text.split()                                            #分词\n",
    "                wordnet_cntent = [wnl.lemmatize(word) for word in tokenized_words]             #词形还原\n",
    "                filtered_words = [word.lower() for word in wordnet_cntent if word.lower() not in stop_words] #小写同时去除停用词\n",
    "                writer.writerow([id_column, filtered_words, label_column])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已处理 {count} 条数据')\n",
    "            print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58b2f58-bc9e-4423-b1cb-2ff22d3e17d7",
   "metadata": {},
   "source": [
    "英文数据处理（无标签）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d12bf2d-c766-4e51-b558-25bcb01ba6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_nolabel(input_file_path, output_file_path, stop_words_file_path):\n",
    "    stop_words = load_stop_words(stop_words_file_path)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        original_header = next(reader)                                                           #读取输入文件表头\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerow(['ID', 'Content', 'Label'])                                          #写入表头\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                id_column = row[0]\n",
    "                content_column = row[1]\n",
    "\n",
    "                text = re.sub(r'<.*?>', '', content_column)\n",
    "                load_text = re.sub(r'[^a-zA-Z\\s]', '', text) #去除非字母和空格的字符\n",
    "                chkr = SpellChecker(\"en_US\")\n",
    "                chkr.set_text(load_text)                                   #拼写检查纠正\n",
    "                for err in chkr:\n",
    "                    load_text = load_text.replace(err.word, err.suggest()[0] if err.suggest() else err.word)\n",
    "                #stemmer = SnowballStemmer(\"english\")\n",
    "                #stemmer_content = stemmer.stem(content_column)                                #词干提取\n",
    "                tokenized_words = load_text.split()                                            #分词\n",
    "                wordnet_cntent = [wnl.lemmatize(word) for word in tokenized_words]             #词形还原\n",
    "                filtered_words = [word.lower() for word in wordnet_cntent if word.lower() not in stop_words] #小写同时去除停用词\n",
    "                writer.writerow([id_column, filtered_words])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已处理 {count} 条数据')\n",
    "            print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11da4f39-448e-40d7-8300-e239d9db5dc6",
   "metadata": {},
   "source": [
    "将处理好的.csv文件转换为.txt文件用于训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c0e8155-414a-4a73-827b-7485d66591f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_text(input_csv_file, output_text_file):\n",
    "    with open(input_csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        header = next(reader)                                                    #跳过表头\n",
    "        with open(output_text_file, 'w', encoding='utf-8') as text_file:\n",
    "            for row in reader:\n",
    "                content = row[1]                                                 #假设内容在第二列\n",
    "                text_file.write(content.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074602d6-12ff-492c-8c79-3c3d26345277",
   "metadata": {},
   "source": [
    "训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c9dca5-59e7-48f2-ac36-509ce66df443",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vector_model(input_file_name,model_file_name):\n",
    "    sentences = word2vec.LineSentence(input_file_name)\n",
    "    #word2vec模型参量的设置\n",
    "    model = word2vec.Word2Vec(sentences,\n",
    "                vector_size=300,          # 词向量长度为300\n",
    "                window=5,                 #表示当前词与预测词在一个句子中的最大距离是多少\n",
    "                min_count=2,              #词频阈值，词出现的频率低于这个次数将不予保存\n",
    "                sg=1,                     #1是skip-gram，0是CBOW\n",
    "                hs=0,                     #1是hierarchical-softmax，0是negative sampling。\n",
    "                                          # hierarchical-softmax本质是把 N 分类问题变成 log(N)次二分类 \n",
    "                                          # negative sampling本质是预测总体类别的一个子集\n",
    "                                          # 二者均属于模型的训练技巧\n",
    "                negative=5,               # 负样例的个数\n",
    "                workers=multiprocessing.cpu_count())   #使用多线程进行处理\n",
    "    model.save(model_file_name)             #保存模型\n",
    "    print(\"训练模型结束...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4f05b-d941-49ce-8418-be5f9c06a556",
   "metadata": {},
   "source": [
    "将句子转化为一个固定长度的向量，返回句子的固定长度特征向量（如平均词向量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94c6d5e2-6c36-4165-87ba-0c987010f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)                                 #取均值作为句子向量\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)                              #如果句子中没有已训练的词，返回零向量"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dd00c10-805f-411f-aab0-a5ba873a51d3",
   "metadata": {},
   "source": [
    "提取数据的特征向量，即每个评论内容的句子向量，并将数据存储于feature.npy和label.npy\n",
    "Args:\n",
    "    data_file: 数据集的文件路径，每行是一个评论，输入格式为.csv\n",
    "    model: 训练好的 Word2Vec 模型\n",
    "Returns:\n",
    "    一个二维数组，表示所有评论的特征向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ae3a1-6798-4bec-b3ee-3a56333e2803",
   "metadata": {},
   "source": [
    "训练集提取特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "132bd3db-7a0a-4014-b905-200084db1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_file, model_file, output_file_features):\n",
    "    model = word2vec.Word2Vec.load(model_file)         #加载词向量模型\n",
    "    features = []\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)                                   #跳过表头\n",
    "        for row in reader:\n",
    "            content = row[1]                           #评论内容#            \n",
    "            vector = get_sentence_vector(content, model)\n",
    "            features.append(vector) \n",
    "    features = np.array(features)                      #转换为 NumPy 数组\n",
    "    \n",
    "    #创建目录（如果它不存在）\n",
    "    output_dir_features = os.path.dirname(output_file_features)\n",
    "    if output_dir_features and not os.path.exists(output_dir_features):\n",
    "        os.makedirs(output_dir_features)\n",
    "\n",
    "    np.save(output_file_features, features)            #保存为.npy文件\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3f9a3-0443-4484-bc66-22fc46ac47c5",
   "metadata": {},
   "source": [
    "训练集提取标签向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d557b3f9-32fe-4174-b012-325beea75fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(data_file, model_file, output_file_labels):\n",
    "    model = word2vec.Word2Vec.load(model_file)         #加载词向量模型\n",
    "    labels = []\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)                                   #跳过表头\n",
    "        for row in reader:       \n",
    "            label = int(row[2])                        #标签\n",
    "            labels.append(label)    \n",
    "    labels = np.array(labels)                          #转换为 NumPy 数组\n",
    "    \n",
    "    #创建目录（如果它不存在）\n",
    "    output_dir_labels = os.path.dirname(output_file_labels)\n",
    "    if output_dir_labels and not os.path.exists(output_dir_labels):\n",
    "        os.makedirs(output_dir_labels)\n",
    "\n",
    "    np.save(output_file_labels, labels)                #保存为.npy文件\n",
    "    return labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a06ddf-4bfe-4896-a4d0-ab2645c02221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative directory: ..\\out\\test_label_data\n",
      "Full output directory: ./out\\..\\out\\test_label_data\n",
      "Output file path: ./out\\..\\out\\test_label_data\\test.label.en.output.csv\n",
      "处理完成！\n",
      "训练模型结束...\n",
      "测试带标签集特征维度： (2500, 300)\n"
     ]
    }
   ],
   "source": [
    "stop_words_file_path = './en_stopwords.txt' \n",
    "\n",
    "model_file_name_train = './wordvec_train_en.model'\n",
    "model_file_name_test_label = './wordvec_testlabel_en.model'\n",
    "model_file_name_test = './wordvec_test_en.model'\n",
    "\n",
    "train_output_file_features = '../files/train_en_features.npy'\n",
    "train_output_file_labels = '../files/train_en_labels.npy'\n",
    "test_label_output_file_features = '../files/test_label_en_features.npy'\n",
    "test_label_output_file_labels = '../files/test_label_en_labels.npy'\n",
    "test_output_file_features = '../files/test_en_features.npy'\n",
    "\n",
    "#train_input_file_name = './out/train_data/en_sample_data/sample.csv' #输入文件路径\n",
    "test_label_input_file_name = './out/test_label_data/test.label.en.csv'\n",
    "#test_input_file_name = './out/test_data/test.en.csv'\n",
    "\n",
    "#train_output_file_path = process_csv_out_path(train_input_file_name)\n",
    "test_label_output_file_path = process_csv_out_path(test_label_input_file_name)\n",
    "#test_output_file_path = process_csv_out_path(test_input_file_name)\n",
    "\n",
    "#train_output_text_path=csv_to_txt_path(train_output_file_path)\n",
    "test_label_output_text_path=csv_to_txt_path(test_label_output_file_path)\n",
    "#test_output_text_path=csv_to_txt_path(test_output_file_path)\n",
    "\n",
    "#english_label(train_input_file_name, train_output_file_path, stop_words_file_path)\n",
    "english_label(test_label_input_file_name, test_label_output_file_path, stop_words_file_path)\n",
    "#english_nolabel(test_input_file_name, test_output_file_path, stop_words_file_path)\n",
    "\n",
    "#csv_to_text(train_output_file_path,train_output_text_path)\n",
    "csv_to_text(test_label_output_file_path,test_label_output_text_path)\n",
    "#csv_to_text(test_output_file_path,test_output_text_path)\n",
    "\n",
    "#word2vector_model(train_output_text_path, model_file_name_train)\n",
    "word2vector_model(test_label_output_text_path, model_file_name_test_label)\n",
    "#word2vector_model(test_output_text_path, model_file_name_test)\n",
    "\n",
    "#train_features = extract_features(train_output_file_path, model_file_name_train, train_output_file_features)\n",
    "#train_labels = extract_labels(train_output_file_path, model_file_name_train, train_output_file_labels)\n",
    "testlabel_features = extract_features(test_label_output_file_path, model_file_name_test_label, test_label_output_file_features)\n",
    "testlabel_labels = extract_labels(test_label_output_file_path, model_file_name_test_label, test_label_output_file_labels)\n",
    "#test_features = extract_features(test_output_file_path, model_file_name_test, test_output_file_features)\n",
    "\n",
    "#print(\"训练集特征维度：\", train_features.shape)\n",
    "print(\"测试带标签集特征维度：\", testlabel_features.shape)\n",
    "#print(\"测试集特征维度：\", test_features.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af72a391-6b77-4d95-a9a3-1ffdf7cc5ef2",
   "metadata": {},
   "source": [
    "stop_words_file_path = './cn_stopwords.txt' \n",
    "\n",
    "model_file_name_train = './wordvec_train_cn.model'\n",
    "model_file_name_test_label = './wordvec_testlabel_cn.model'\n",
    "model_file_name_test = './wordvec_test_cn.model'\n",
    "\n",
    "train_output_file_features = '../files/train_cn_features.npy'\n",
    "train_output_file_labels = '../files/train_cn_labels.npy'\n",
    "test_label_output_file_features = '../files/test_label_cn_features.npy'\n",
    "test_label_output_file_labels = '../files/test_label_cn_labels.npy'\n",
    "test_output_file_features = '../files/test_cn_features.npy'\n",
    "\n",
    "train_input_file_name = './out/train_data/cn_sample_data/sample.csv' #输入文件路径\n",
    "test_label_input_file_name = './out/test_label_data/test.label.cn.csv'\n",
    "test_input_file_name = './out/test_data/test.cn.csv'\n",
    "\n",
    "train_output_file_path = process_csv_out_path(train_input_file_name)\n",
    "test_label_output_file_path = process_csv_out_path(test_label_input_file_name)\n",
    "test_output_file_path = process_csv_out_path(test_input_file_name)\n",
    "\n",
    "train_output_text_path=csv_to_txt_path(train_output_file_path)\n",
    "test_label_output_text_path=csv_to_txt_path(test_label_output_file_path)\n",
    "test_output_text_path=csv_to_txt_path(test_output_file_path)\n",
    "\n",
    "word2vector_label(train_input_file_name, train_output_file_path, stop_words_file_path)\n",
    "word2vector_label(test_label_input_file_name, test_label_output_file_path, stop_words_file_path)\n",
    "word2vector_nolabel(test_input_file_name, test_output_file_path, stop_words_file_path)\n",
    "\n",
    "csv_to_text(train_output_file_path,train_output_text_path)\n",
    "csv_to_text(test_label_output_file_path,test_label_output_text_path)\n",
    "csv_to_text(test_output_file_path,test_output_text_path)\n",
    "\n",
    "word2vector_model(train_output_text_path, model_file_name_train)\n",
    "word2vector_model(test_label_output_text_path, model_file_name_test_label)\n",
    "word2vector_model(test_output_text_path, model_file_name_test)\n",
    "\n",
    "train_features = extract_features(train_output_file_path, model_file_name_train, train_output_file_features)\n",
    "train_labels = extract_labels(train_output_file_path, model_file_name_train, train_output_file_labels)\n",
    "testlabel_features = extract_features(test_label_output_file_path, model_file_name_test_label, test_label_output_file_features)\n",
    "testlabel_labels = extract_labels(test_label_output_file_path, model_file_name_test_label, test_label_output_file_labels)\n",
    "test_features = extract_features(test_output_file_path, model_file_name_test, test_output_file_features)\n",
    "\n",
    "print(\"训练集特征维度：\", train_features.shape)\n",
    "print(\"测试带标签集特征维度：\", testlabel_features.shape)\n",
    "print(\"测试集特征维度：\", test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f61050-2ebc-43be-b6b7-eabccdfcda32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

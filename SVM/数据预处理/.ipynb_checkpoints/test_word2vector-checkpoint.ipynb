{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f0e0f-5ac2-4f90-85ef-546cea095fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zhconv                          #导入zhconv模块\n",
    "import jieba                           #导入jieba分词模块\n",
    "import re                              #导入正则表达式模块\n",
    "import multiprocessing                 #导入多进程模块\n",
    "from pprint import pprint\n",
    "from utils.configs import csv_to_txt_path\n",
    "from utils.configs import process_csv_out_path\n",
    "from gensim.corpora import WikiCorpus    #导入Wiki语料库\n",
    "from gensim.models import word2vec      #导入word2vec模型\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d890b-ff5e-4289-8e0a-fa13774a07f3",
   "metadata": {},
   "source": [
    "加载停用词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5809bf4-5b55-4307-8cad-c5ad028689df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(word.strip() for word in file.readlines())\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a5d579-62b6-4c13-b24a-efc2c7484008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, stop_words):\n",
    "    text = text.strip().replace(\"\\n\", \"\").replace(\"\\r\", \"\")                                      #去除换行符和多余空格\n",
    "    words = jieba.cut(text)                                                                      #使用jieba分词\n",
    "    processed_words = [word for word in words if word not in stop_words and word.strip() != \"\"]  #去除停用词\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6b533d-00bf-4b67-84d2-53e07c654856",
   "metadata": {},
   "source": [
    "中文数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a2017-99e1-4991-bbf6-a225ea2687ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vector(input_file_path, output_file_path, stop_words_file_path):\n",
    "    cn_reg = '[\\u4e00-\\u9fa5]'                                                                   #正则表达式，匹配所有中文字符\n",
    "    stop_words = load_stop_words(stop_words_file_path)                                           #加载停用词    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        original_header = next(reader)                                                           #读取输入文件表头\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerow(['ID', 'Content'])                                          #写入表头\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                id_column = row[0]\n",
    "                content_column = row[1]\n",
    "                \n",
    "                simplified_content = zhconv.convert(content_column, 'zh-hans')                   #将内容转换为简体字\n",
    "                segmented_content = jieba.cut(simplified_content)                                #分词处理\n",
    "                filtered_words = [word for word in segmented_content if re.match(cn_reg, word)]  #去除非中文字符，仅保留中文字符\n",
    "                #processed_content = [word for word in filtered_words if word not in stop_words]  #去停用词\n",
    "                cleaned_content = ' '.join(filtered_words)                                       #合并为字符串\n",
    "                \n",
    "                writer.writerow([id_column, cleaned_content])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已处理 {count} 条数据')\n",
    "            print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14e555-c314-4925-b496-4b6c06cb412b",
   "metadata": {},
   "source": [
    "英文数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6111179-1290-4fa2-8428-b5d2e71fe820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0768352d-cb3a-4e2a-9692-be41594ab4fb",
   "metadata": {},
   "source": [
    "将处理好的.csv文件转换为.txt文件用于训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcea1a-9963-405a-9aaf-3d7bfa7f7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_text(input_csv_file, output_text_file):\n",
    "    with open(input_csv_file, 'r', encoding='utf-8') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "        header = next(reader)                                                    #跳过表头\n",
    "        with open(output_text_file, 'w', encoding='utf-8') as text_file:\n",
    "            for row in reader:\n",
    "                content = row[1]                                                 #假设内容在第二列\n",
    "                text_file.write(content.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e52b9-c976-497e-95a0-ffadac714133",
   "metadata": {},
   "source": [
    "训练词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981520db-6599-4fb8-b379-a49c2729fa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vector_model(input_file_name,model_file_name):\n",
    "    sentences = word2vec.LineSentence(input_file_name)\n",
    "    #word2vec模型参量的设置\n",
    "    model = word2vec.Word2Vec(sentences,\n",
    "                vector_size=300,          # 词向量长度为300\n",
    "                window=5,                 #表示当前词与预测词在一个句子中的最大距离是多少\n",
    "                min_count=5,\n",
    "                sg=0,                     #1是skip-gram，0是CBOW\n",
    "                hs=0,                     #1是hierarchical-softmax，0是negative sampling。\n",
    "                                          # hierarchical-softmax本质是把 N 分类问题变成 log(N)次二分类 \n",
    "                                          # negative sampling本质是预测总体类别的一个子集\n",
    "                                          # 二者均属于模型的训练技巧\n",
    "                negative=5,               # 负样例的个数\n",
    "                workers=multiprocessing.cpu_count())   #使用多线程进行处理\n",
    "    model.save(model_file_name)             #保存模型\n",
    "    print(\"训练模型结束...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836a5d16-6c31-45dc-a17e-ad5966f61714",
   "metadata": {},
   "source": [
    "将句子转化为一个固定长度的向量，返回句子的固定长度特征向量（如平均词向量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c36529-10cc-4fa0-8f8f-6fa30e5c62ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, model):\n",
    "    vectors = [model.wv[word] for word in sentence if word in model.wv]\n",
    "    if len(vectors) > 0:\n",
    "        return np.mean(vectors, axis=0)                                 #取均值作为句子向量\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)                              #如果句子中没有已训练的词，返回零向量"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e41593fd-4549-4652-a26b-979886096e14",
   "metadata": {},
   "source": [
    "提取数据的特征向量，即每个评论内容的句子向量，并将数据存储于feature.npy和label.npy\n",
    "Args:\n",
    "    data_file: 数据集的文件路径，每行是一个评论，输入格式为.csv\n",
    "    model: 训练好的 Word2Vec 模型\n",
    "Returns:\n",
    "    一个二维数组，表示所有评论的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fdeb11-cacb-4065-858e-e80802af83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "测试集提取特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145293b-dd6a-4dda-b38a-fc120dd48904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data_file, model_file, output_file_features):\n",
    "    model = word2vec.Word2Vec.load(model_file)         #加载词向量模型\n",
    "    features = []\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)                                   #跳过表头\n",
    "        for row in reader:\n",
    "            content = row[1]                           #评论内容#            \n",
    "            vector = get_sentence_vector(content, model)\n",
    "            features.append(vector)\n",
    "    features = np.array(features)                      #转换为 NumPy 数组\n",
    "    \n",
    "    #创建目录（如果它不存在）\n",
    "    output_dir_features = os.path.dirname(output_file_features)\n",
    "    if output_dir_features and not os.path.exists(output_dir_features):\n",
    "        os.makedirs(output_dir_features)\n",
    "        \n",
    "    np.save(output_file_features, features)            #保存为.npy文件\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e97a2f1-1bcd-446f-86ed-41be80789f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file_path = './stopword.txt' \n",
    "model_file_name = './wordvec.cn.test.model'\n",
    "output_file_features = '../files/test_cn_features.npy'\n",
    "input_file_name = './out/test_data/test.cn.csv' #输入文件路径\n",
    "output_file_path = process_csv_out_path(input_file_name)\n",
    "output_text_path=csv_to_txt_path(output_file_path)\n",
    "word_to_vector(input_file_name, output_file_path, stop_words_file_path)\n",
    "csv_to_text(output_file_path,output_text_path)\n",
    "word2vector_model(output_text_path, model_file_name)\n",
    "test_features= extract_features(output_file_path, model_file_name, output_file_features)\n",
    "print(\"训练集特征维度：\", test_features.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b502585d-6b6a-4f3a-ac77-e447bfec444f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus    #导入Wiki语料库\n",
    "from gensim.models import word2vec      #导入word2vec模型\n",
    "import zhconv                          #导入zhconv模块\n",
    "import jieba                           #导入jieba分词模块\n",
    "import re                              #导入正则表达式模块\n",
    "import multiprocessing                 #导入多进程模块\n",
    "import csv\n",
    "from utils.configs import convert_to_simplified_output_path\n",
    "from utils.configs import remove_non_chinese_output_path\n",
    "from utils.configs import process_csv_out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbf2586-c2fd-4d44-88a7-f1c1700566ef",
   "metadata": {},
   "source": [
    "读取文件并处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3460636a-9eef-48eb-830e-6112b2bd7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_chinese(input_file_path, output_file_path):\n",
    "    # 正则表达式，匹配所有中文字符\n",
    "    cn_reg = '[\\u4e00-\\u9fa5]'  \n",
    "    \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                id_column = row[0]\n",
    "                content_column = row[1]\n",
    "                \n",
    "                # 将内容转换为简体字\n",
    "                simplified_content = zhconv.convert(content_column, 'zh-hans')\n",
    "                # 去除非中文字符，仅保留中文字符\n",
    "                cleaned_content = ''.join(re.findall(cn_reg, simplified_content))\n",
    "                # 分词处理\n",
    "                segmented_content = ' '.join(jieba.cut(cleaned_content))\n",
    "\n",
    "                writer.writerow([id_column, segmented_content])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已处理 {count} 条数据')\n",
    "            print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458169f9-1da6-4433-ac19-e6f5f9b5a4d0",
   "metadata": {},
   "source": [
    "加载停用词列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f29007fb-a97d-4433-b9cc-2f43ef8a0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(word.strip() for word in file.readlines())\n",
    "#        stop_words = set(file.read().splitlines())  # 读取停用词文件，返回一个集合\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16ba1373-57c1-41e0-9f45-d6a86f8904bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, stop_words):\n",
    "    # 去除换行符和多余空格\n",
    "    text = text.strip().replace(\"\\n\", \"\").replace(\"\\r\", \"\")\n",
    "    # 使用 jieba 分词\n",
    "    words = jieba.cut(text)\n",
    "    # 去除停用词\n",
    "    processed_words = [word for word in words if word not in stop_words and word.strip() != \"\"]\n",
    "    return \" \".join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5c6845c-ea8a-4ec6-bd50-91f2785afe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理 CSV 文件\n",
    "def process_csv(input_file_path, output_file_path, stop_words_file_path):\n",
    "    stop_words = load_stop_words(stop_words_file_path)  # 加载停用词\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)   \n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                # 假设第一列是ID，第二列是内容\n",
    "                id_column = row[0]  # 获取ID列\n",
    "                content_column = row[1]  # 获取内容列\n",
    "                # 对内容进行预处理\n",
    "                processed_content = preprocess_text(content_column, stop_words)\n",
    "                # 写入 ID 和处理后的内容\n",
    "                writer.writerow([id_column, processed_content])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已处理 {count} 条数据')\n",
    "            print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6d7bcd5-6ef5-49d5-8d0a-3142fc7af618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output file path: ./out/train_data/cn_sample_data\\sample.negative.c2s.rcn.csv\n",
      "Relative directory: ..\\out\\train_data\\cn_sample_data\n",
      "Full output directory: ./files\\..\\out\\train_data\\cn_sample_data\n",
      "Output file path: ./files\\..\\out\\train_data\\cn_sample_data\\sample.negative.c2s.rcn.output.csv\n",
      "c2s_file_name: ./out/train_data/cn_sample_data\\sample.negative.c2s.csv\n",
      "rcn_file_name: ./out/train_data/cn_sample_data\\sample.negative.c2s.rcn.csv\n",
      "output_file_path: ./files\\..\\out\\train_data\\cn_sample_data\\sample.negative.c2s.rcn.output.csv\n",
      "转换完成！\n",
      "处理完成！\n",
      "处理完成！\n"
     ]
    }
   ],
   "source": [
    "# 输入文件路径\n",
    "input_file_name = './out/train_data/cn_sample_data/sample.negative.csv'\n",
    "\n",
    "# 生成路径并确保输出\n",
    "c2s_file_name = convert_to_simplified_output_path(input_file_name)\n",
    "rcn_file_name = remove_non_chinese_output_path(c2s_file_name)\n",
    "stop_words_file_path = './stopword.txt'  # 停用词文件路径\n",
    "output_file_path = process_csv_out_path(rcn_file_name)\n",
    "\n",
    "# 输出调试信息，确保路径正确\n",
    "print(f\"c2s_file_name: {c2s_file_name}\")\n",
    "print(f\"rcn_file_name: {rcn_file_name}\")\n",
    "print(f\"output_file_path: {output_file_path}\")\n",
    "\n",
    "# 调用处理函数，确保文件生成\n",
    "convert_to_simplified(input_file_name, c2s_file_name)\n",
    "remove_non_chinese(c2s_file_name, rcn_file_name)\n",
    "process_csv(rcn_file_name, output_file_path, stop_words_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112cfa90-88d4-46b9-910a-b49a9f73aab7",
   "metadata": {},
   "source": [
    "转换繁体字为简体字"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7718987-0b6a-403a-88be-42800c597a57",
   "metadata": {},
   "source": [
    "def convert_to_simplified(input_file_path, output_file_path):\n",
    "    with open(input_file_name, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        # 读取输入文件表头\n",
    "        original_header = next(reader)\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            # 写入表头\n",
    "            writer.writerow(['ID', 'Content'])            \n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                # 假设第一列是ID，第二列是内容\n",
    "                id_column = row[0]\n",
    "                content_column = row[1]\n",
    "                \n",
    "                # 将内容转换为简体字\n",
    "                simplified_content = zhconv.convert(content_column, 'zh-hans')\n",
    "                \n",
    "                # 将内容写入输出文件\n",
    "                writer.writerow([id_column, simplified_content])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已转换{count}条数据')\n",
    "            print('转换完成！')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b849f-79c8-4bc3-bfae-810ff477abef",
   "metadata": {},
   "source": [
    "分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb17977d-57e6-4f86-8664-547ea63f851a",
   "metadata": {},
   "source": [
    "input_file_name = 'corpus_cn_simple.txt'\n",
    "output_file_name = 'corpus_cn_simple_separate.txt'\n",
    "#读取文件\n",
    "with open(input_file_name, 'r', encoding='utf-8') as input_file:   \n",
    "    lines = input_file.readlines()\n",
    "    count = 0\n",
    "    with open(output_file_name, 'w', encoding='utf-8') as output_file:\n",
    "        for line in lines:\n",
    "            # jieba分词的结果是一个list，需要拼接，但是jieba把空格回车都当成一个字符处理\n",
    "            output_file.write(' '.join(jieba.cut(line.split('\\n')[0].replace(' ', ''))) + '\\n')\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print('已分词%d条数据' % count)\n",
    "    print('处理完成！')\n",
    "#查看结果\n",
    "with open('corpus_cn_simple_separate.txt',\"r\",encoding=\"utf8\") as f:\n",
    "    print(f.readlines()[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9c8c6-d720-442a-a1f8-5bf9ba9f8906",
   "metadata": {},
   "source": [
    "去除非中文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f732c40-e33b-4942-b978-511fe533a818",
   "metadata": {},
   "source": [
    "def remove_non_chinese(input_file_path, output_file_path):\n",
    "    cn_reg = '^[\\u4e00-\\u9fa5]+$'  # 正则表达式，匹配中文\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "        reader = csv.reader(input_file)\n",
    "        with open(output_file_path, 'w', encoding='utf-8', newline='') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            count = 0\n",
    "            for row in reader:\n",
    "                id_column = row[0]  # 获取ID列\n",
    "                content_column = row[1]  # 获取内容列\n",
    "                # 分割内容为单词，并移除非中文的词\n",
    "                line_list = content_column.split()  # 假设内容是由空格分隔\n",
    "                line_list_new = []\n",
    "                for word in line_list:\n",
    "                    if re.match(cn_reg, word):  # 只保留完全是中文的词\n",
    "                        line_list_new.append(word)\n",
    "                # 将ID和处理后的内容写入输出文件\n",
    "                writer.writerow([id_column, ' '.join(line_list_new)])\n",
    "                count += 1\n",
    "                if count % 10000 == 0:\n",
    "                    print(f'已处理 {count} 条数据')\n",
    "            print(\"处理完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df2d02-4a47-4f31-b07c-10056472d6d4",
   "metadata": {},
   "source": [
    "训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279733f-4e6b-4f02-b78b-83827a40231a",
   "metadata": {},
   "source": [
    "input_file_name = 'corpus.txt'\n",
    "model_file_name = 'wordvec.model'\n",
    "\n",
    "sentences = word2vec.LineSentence(input_file_name)\n",
    "#word2vec模型参量的设置\n",
    "model = word2vec.Word2Vec(sentences,\n",
    "            vector_size=300,                  # 词向量长度为300\n",
    "            window=5,                 #表示当前词与预测词在一个句子中的最大距离是多少\n",
    "            min_count=5,\n",
    "            sg=0,                     #1是skip-gram，0是CBOW\n",
    "            hs=0,                     #1是hierarchical-softmax，0是negative sampling。\n",
    "                                      # hierarchical-softmax本质是把 N 分类问题变成 log(N)次二分类 \n",
    "                                      # negative sampling本质是预测总体类别的一个子集\n",
    "                                      # 二者均属于模型的训练技巧\n",
    "            negative=5,                # 负样例的个数\n",
    "            workers=multiprocessing.cpu_count())   #使用多线程进行处理\n",
    "\n",
    "model.save(model_file_name)             #保存模型\n",
    "print(\"训练模型结束...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e472224-0a50-4792-b1bc-a6e19b6520ab",
   "metadata": {},
   "source": [
    "加载模型并测试效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6058daeb-ea07-4a13-88ff-286218cf3d2f",
   "metadata": {},
   "source": [
    "model_path = \"wordvec.model\"\n",
    "wordvec = word2vec.Word2Vec.load(model_path)\n",
    "#获得\"华为\"的词向量\n",
    "wordvec.wv.get_vector(\"华为\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72be7998-84c0-4811-bf5e-c2539ee2abc7",
   "metadata": {},
   "source": [
    "def process_data(input_file_name, output_file_name):\n",
    "    with open(input_file_name, 'r', encoding=\"utf8\") as input_file, open(output_file_name, 'w', encoding=\"utf8\") as output_file:\n",
    "        print(\"开始处理数据\")\n",
    "        reader = csv.reader(input_file)\n",
    "        writer = csv.writer(output_file)\n",
    "        count = 0\n",
    "        \n",
    "        for row in reader:\n",
    "            # 假设第一列是ID，第二列是要处理的内容\n",
    "            id_column = row[0]  # 第一列是ID\n",
    "            second_column = row[1].strip() # 第二列去掉空格等\n",
    "            # 在这里进行你想要的操作，比如将第二列内容转小写\n",
    "            processed_content = second_column # 你可以根据需要修改这里的操作\n",
    "            # 写入处理后的数据，保持ID列和处理后的第二列\n",
    "            writer.writerow([id_column, processed_content])        \n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f'已处理{count}条数据')\n",
    "        print('处理完成！')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
